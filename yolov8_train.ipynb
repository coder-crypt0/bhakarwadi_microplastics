{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1cc4d39",
   "metadata": {},
   "source": [
    "# YOLOv8 Training Notebook (Google Colab)\n",
    "This notebook helps you upload the dataset assets to Google Colab (or mount Google Drive), prepare the YOLOv8 dataset layout, and train a YOLOv8 model using the GPU.\n",
    "\n",
    "What this notebook does:\n",
    "- Checks GPU availability and PyTorch/Ultralytics install\n",
    "- Installs Ultralytics (YOLOv8) and required packages\n",
    "- Lets you either mount Google Drive or upload dataset zip files directly via the file picker\n",
    "- Unzips and arranges the dataset into the YOLOv8 expected structure (images/labels split)\n",
    "- Writes a `data.yaml` describing the dataset for YOLOv8\n",
    "- Launches training with the Ultralytics Python API\n",
    "\n",
    "Notes:\n",
    "- In Colab, select Runtime -> Change runtime type -> GPU.\n",
    "- For large datasets prefer mounting Google Drive to avoid repeated uploads and preserve outputs.\n",
    "- Edit the training cell parameters (epochs, batch, imgsz) to match your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8809488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Google Colab and GPU availability\n",
    "import sys\n",
    "in_colab = 'google.colab' in sys.modules\n",
    "print('Running in Colab:' , in_colab)\n",
    "\n",
    "# Show GPU status (works in Colab)\n",
    "try:\n",
    "    import torch\n",
    "    print('torch.__version__ =', torch.__version__)\n",
    "    print('cuda available =', torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print('cuda device count =', torch.cuda.device_count())\n",
    "except Exception as e:\n",
    "    print('Torch not available yet:', e)\n",
    "\n",
    "# Try nvidia-smi (only works if driver present, e.g., Colab GPU runtime)\n",
    "import os\n",
    "if os.name == 'posix':\n",
    "    try:\n",
    "        !nvidia-smi\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54186aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install/upgrade Ultralytics (YOLOv8) and unzip helper (runs in Colab)\n",
    "# This cell is safe to re-run; it will reinstall if updates are available.\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    print('Installing/Upgrading ultralytics...')\n",
    "    !pip install -U ultralytics --quiet\n",
    "    !pip install -q wandb\n",
    "else:\n",
    "    print('Not in Colab; please ensure you have ultralytics installed locally (pip install -U ultralytics).')\n",
    "\n",
    "# Show installed ultralytics version\n",
    "try:\n",
    "    import ultralytics\n",
    "    print('ultralytics version =', ultralytics.__version__)\n",
    "except Exception as e:\n",
    "    print('Ultralytics not available yet:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8781ba2f",
   "metadata": {},
   "source": [
    "## Upload dataset assets or mount Google Drive\n",
    "Choose one of the two options below:\n",
    "1) Mount Google Drive (recommended for large datasets / to persist outputs)\n",
    "2) Use the file uploader to upload the dataset zip files directly to the Colab VM.\n",
    "\n",
    "You should have these files (from this repository `datasets/`):\n",
    "- train_images.zip\n",
    "- train_labels.zip\n",
    "- val_images.zip\n",
    "- val_labels.zip\n",
    "- test_images.zip (optional)\n",
    "- test_labels.zip (optional)\n",
    "\n",
    "The next two cells provide both flows. Run the one you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008a7706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Mount Google Drive (uncomment and run if you want persistence)\n",
    "# This will create /content/drive/MyDrive/microplastic_dataset where you can upload the zip files via Drive UI or the Drive web interface.\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# dataset_dir = '/content/drive/MyDrive/microplastic_dataset'\n",
    "# print('Dataset dir (Drive):', dataset_dir)\n",
    "\n",
    "print('Option A: mount Google Drive is available but commented out. If you prefer Drive, uncomment the lines and run this cell.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a35d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: Upload dataset zip files directly to the Colab VM using the upload widget\n",
    "# This is convenient for small/medium sized zips. Uploaded files are written to /content/datasets/\n",
    "import os\n",
    "os.makedirs('/content/datasets', exist_ok=True)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    for fname, data in uploaded.items():\n",
    "        out_path = os.path.join('/content/datasets', fname)\n",
    "        with open(out_path, 'wb') as f:\n",
    "            f.write(data)\n",
    "        print('Saved', out_path)\n",
    "except Exception as e:\n",
    "    print('files.upload is only available in Colab. If you are running locally copy the dataset zips into /content/datasets or mount Drive. Error:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac0c7a9",
   "metadata": {},
   "source": [
    "## Prepare dataset layout for YOLOv8\n",
    "The code below expects the image zips and label zips to be present in `/content/datasets`. It will: \n",
    "- extract the zips into `/content/datasets/raw`\n",
    "- place images into `/content/datasets/images/{train,val,test}` and labels into `/content/datasets/labels/{train,val,test}`\n",
    "- write a `data.yaml` at `/content/datasets/data.yaml` suitable for YOLOv8 training.\n",
    "\n",
    "If your zips already contain folder structures, the script will try to locate images (.jpg/.png) and label files (.txt) and move them appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f9aa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip and organize dataset files into YOLOv8 expected layout\n",
    "import os, zipfile, shutil, glob\n",
    "base = '/content/datasets'\n",
    "raw = os.path.join(base, 'raw')\n",
    "os.makedirs(raw, exist_ok=True)\n",
    "# Extract any zip files found in /content/datasets\n",
    "for z in glob.glob(os.path.join(base, '*.zip')):\n",
    "    print('Extracting', z)\n",
    "    try:\n",
    "        with zipfile.ZipFile(z, 'r') as zip_ref:\n",
    "            zip_ref.extractall(raw)\n",
    "    except Exception as e:\n",
    "        print('Failed to extract', z, e)\n",
    "\n",
    "# Create expected folders\n",
    "images_dir = os.path.join(base, 'images')\n",
    "labels_dir = os.path.join(base, 'labels')\n",
    "for subset in ['train','val','test']:\n",
    "    os.makedirs(os.path.join(images_dir, subset), exist_ok=True)\n",
    "    os.makedirs(os.path.join(labels_dir, subset), exist_ok=True)\n",
    "\n",
    "# Helper to move files by extension from raw to target folder\n",
    "def move_files(patterns, target_dir):\n",
    "    moved = 0\n",
    "    for pat in patterns:\n",
    "        for f in glob.glob(os.path.join(raw, '**', pat), recursive=True):\n",
    "            try:\n",
    "                shutil.move(f, target_dir)\n",
    "                moved += 1\n",
    "            except Exception as e:\n",
    "                print('move failed', f, e)\n",
    "    return moved\n",
    "\n",
    "# Try to detect and move images and labels using heuristic names\n",
    "image_patterns = ['*.jpg','*.jpeg','*.png','*.bmp']\n",
    "label_patterns = ['*.txt']\n",
    "\n",
    "# Heuristics: if raw contains folders named \n",
    ", \n",
    ", \n",
    " move accordingly, else distribute by file names if they contain \n",
    "/\n",
    "/\n",
    "\n",
    "for subset in ['train','val','test']:\n",
    "    # move images whose path contains subset\n",
    "    moved_imgs = 0\n",
    "    moved_lbls = 0\n",
    "    for pat in image_patterns:\n",
    "        for f in glob.glob(os.path.join(raw, '**', '*'+subset+'*', pat), recursive=True):\n",
    "            try:\n",
    "                shutil.move(f, os.path.join(images_dir, subset))\n",
    "                moved_imgs += 1\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    for pat in label_patterns:\n",
    "        for f in glob.glob(os.path.join(raw, '**', '*'+subset+'*', pat), recursive=True):\n",
    "            try:\n",
    "                shutil.move(f, os.path.join(labels_dir, subset))\n",
    "                moved_lbls += 1\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    print(f'{subset}: moved images={moved_imgs}, labels={moved_lbls}')\n",
    "\n",
    "# If nothing moved by heuristic, fall back to moving all images to train/val/test by looking at zip filenames\n",
    "if sum(len(os.listdir(os.path.join(images_dir, s))) for s in ['train','val','test']) == 0:\n",
    "    print('Heuristic failed to detect subsets; falling back to zip-name based placement')\n",
    "    # If zip names contained 'train_images' etc., use them\n",
    "    name_map = {'train_images':'train','val_images':'val','test_images':'test', 'train_labels':'train','val_labels':'val','test_labels':'test'}\n",
    "    for z in glob.glob(os.path.join(base, '*.zip')):\n",
    "        zname = os.path.basename(z).lower()\n",
    "        target = None\n",
    "        for key,v in name_map.items():\n",
    "            if key in zname:\n",
    "                target = v\n",
    "                break\n",
    "        if target is None:\n",
    "            continue\n",
    "        # extract this zip to a temp dir and move files by extension\n",
    "        with zipfile.ZipFile(z, 'r') as zip_ref:\n",
    "            zip_ref.extractall(os.path.join(raw, 'tmp_'+os.path.splitext(os.path.basename(z))[0]))\n",
    "        moved_imgs += move_files(image_patterns, os.path.join(images_dir, target))\n",
    "        moved_lbls += move_files(label_patterns, os.path.join(labels_dir, target))\n",
    "    print('Fallback moved imgs, labels:', moved_imgs, moved_lbls)\n",
    "\n",
    "# Final counts\n",
    "for subset in ['train','val','test']:\n",
    "    imgs = len(list(glob.glob(os.path.join(images_dir, subset, '*'))))\n",
    "    lbls = len(list(glob.glob(os.path.join(labels_dir, subset, '*.txt'))))\n",
    "    print(f'Subset {subset}: images={imgs}, labels={lbls}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7c4a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a YOLOv8-compatible data.yaml into /content/datasets/data.yaml\n",
    "import yaml, os\n",
    "base = '/content/datasets'\n",
    "data_yaml = {\n",
    "    'path': base,\n",
    "    'train': 'images/train',\n",
    "    'val': 'images/val',\n",
    "    'test': 'images/test',\n",
    "    'names': ['Microplastic']\n",
    "}\n",
    "with open(os.path.join(base, 'data.yaml'), 'w') as f:\n",
    "    yaml.dump(data_yaml, f, default_flow_style=False)\n",
    "print('Wrote', os.path.join(base, 'data.yaml'))\n",
    "print('Contents:')\n",
    "print(open(os.path.join(base, 'data.yaml')).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3e6de9",
   "metadata": {},
   "source": [
    "## Train YOLOv8\n",
    "The cell below launches training using the Ultralytics YOLOv8 Python API. Adjust `epochs`, `batch`, `imgsz` and `model_size` as needed.\n",
    "- `model_size` defaults to `yolov8s.pt` (small) which trains faster.\n",
    "- `device=0` uses the first CUDA GPU. Use `device='cuda'` for automatic selection.\n",
    "- To persist results, set `project` and `name` to a Drive folder if you mounted Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f426a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training cell - edit hyperparameters before running\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "data_yaml = '/content/datasets/data.yaml'\n",
    "# Choose model_size: 'yolov8n.pt','yolov8s.pt','yolov8m.pt','yolov8l.pt' etc.\n",
    "model_size = 'yolov8s.pt'\n",
    "epochs = 100\n",
    "imgsz = 640\n",
    "batch = 16\n",
    "device = 0  # GPU index (0) - change to 'cpu' to run on CPU\n",
    "project = '/content/runs/YOLOv8'  # change to Drive path if mounted for persistence\n",
    "name = 'microplastic_exp'\n",
    "\n",
    "print('Starting training with', model_size, 'epochs=', epochs, 'batch=', batch)\n",
    "# Create model (loads pretrained weights)\n",
    "model = YOLO(model_size)\n",
    "# Train\n",
    "model.train(data=data_yaml, epochs=epochs, imgsz=imgsz, batch=batch, device=device, project=project, name=name)\n",
    "\n",
    "# After training the best weights are saved under project/name/weights/best.pt\n",
    "print('Training launched. Check the run folder for checkpoints and results.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5503c2ae",
   "metadata": {},
   "source": [
    "## Quick inference example (after training)\n",
    "After training completes, you can run a quick inference using the trained weights. Update the `weights_path` below to the best checkpoint saved by the training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d44afff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick inference example - update weights_path if needed\n",
    "from ultralytics import YOLO\n",
    "weights_path = '/content/runs/YOLOv8/microplastic_exp/weights/best.pt'  # default training save location\n",
    "if os.path.exists(weights_path):\n",
    "    y = YOLO(weights_path)\n",
    "    # run inference on a sample image if present\n",
    "    sample_images = glob.glob('/content/datasets/images/val/*')\n",
    "    if len(sample_images) > 0:\n",
    "        print('Running inference on', sample_images[0])\n",
    "        results = y.predict(sample_images[0], save=True)\n",
    "        print('Saved predictions to /content/runs/predict')\n",
    "    else:\n",
    "        print('No validation images found for inference')\n",
    "else:\n",
    "    print('Weights not found at', weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef462cd5",
   "metadata": {},
   "source": [
    "## Notes & tips\n",
    "- For long trainings or large datasets, mount Google Drive and set `project` to a Drive folder so results are saved persistently.\n",
    "- If you encounter out-of-memory errors, reduce `batch` or `imgsz`, or use a smaller model (yolov8n/yolov8s).\n",
    "- Use `epochs=10` for a quick smoke test before running longer experiments.\n",
    "- To resume training from a checkpoint, pass `resume=True` or set `model = YOLO(checkpoint_path)` and call `model.train(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41cf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained weights and run folder to Google Drive\n",
    "# Run this cell AFTER training (you can queue it and leave Colab running). It will mount Drive (if not already mounted) and copy weights and the whole run folder.\n",
    "import os, shutil, glob, time\n",
    "# Attempt to mount Google Drive if running in Colab\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    print('Mounting Google Drive...')\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    drive_mounted = True\n",
    "except Exception as e:\n",
    "    print('Google Drive mount not available or already handled:', e)\n",
    "    drive_mounted = os.path.exists('/content/drive')\n",
    "\n",
    "# Destination folder in Drive where artifacts will be copied\n",
    "dst_base = '/content/drive/MyDrive/microplastic_yolov8_runs'\n",
    "os.makedirs(dst_base, exist_ok=True)\n",
    "# Default run and weights paths (match training cell defaults)\n",
    "run_folder = '/content/runs/YOLOv8/microplastic_exp'\n",
    "weights_dir = os.path.join(run_folder, 'weights')\n",
    "best_pt = os.path.join(weights_dir, 'best.pt')\n",
    "# Helper to copy with retries (handles transient Drive issues)\n",
    "def copy_with_retries(src, dst, retries=3, wait=3):\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            if os.path.isdir(src):\n",
    "                if os.path.exists(dst):\n",
    "                    shutil.rmtree(dst)\n",
    "                shutil.copytree(src, dst)\n",
    "            else:\n",
    "                shutil.copy2(src, dst)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f'Copy attempt {i+1} failed:', e)\n",
    "            time.sleep(wait)\n",
    "    return False\n",
    "\n",
    "# Copy best.pt if it exists\n",
    "if os.path.exists(best_pt):\n",
    "    print('Found best.pt at', best_pt)\n",
    "    dst_weights = os.path.join(dst_base, 'weights')\n",
    "    os.makedirs(dst_weights, exist_ok=True)\n",
    "    out = copy_with_retries(best_pt, os.path.join(dst_weights, 'best.pt'))\n",
    "    if out:\n",
    "        print('Copied best.pt to', dst_weights)\n",
    "    else:\n",
    "        print('Failed to copy best.pt')\n",
    "else:\n",
    "    # Try to find any .pt files in weights folder\n",
    "    pts = glob.glob(os.path.join(weights_dir, '*.pt'))\n",
    "    if len(pts) > 0:\n",
    "        dst_weights = os.path.join(dst_base, 'weights')\n",
    "        os.makedirs(dst_weights, exist_ok=True)\n",
    "        for p in pts:\n",
    "            print('Copying', p)\n",
    "            copy_with_retries(p, os.path.join(dst_weights, os.path.basename(p)))\n",
    "        print('Copied', len(pts), 'pt files to', dst_weights)\n",
    "    else:\n",
    "        print('No .pt files found at', weights_dir)\n",
    "\n",
    "# Copy entire run folder (checkpoints, logs, plots) to Drive for safety\n",
    "if os.path.exists(run_folder):\n",
    "    dst_run = os.path.join(dst_base, os.path.basename(run_folder))\n",
    "    print('Copying run folder', run_folder, '->', dst_run)\n",
    "    ok = copy_with_retries(run_folder, dst_run)\n",
    "    if ok:\n",
    "        print('Run folder copied successfully to', dst_run)\n",
    "    else:\n",
    "        print('Failed to copy run folder')\n",
    "else:\n",
    "    print('Run folder not found:', run_folder)\n",
    "\n",
    "print('Done. Check your Google Drive at', dst_base)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
